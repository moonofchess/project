{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "062104df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "from IPython.display import clear_output\n",
    "import chromedriver_autoinstaller\n",
    "from selenium import webdriver\n",
    "from tqdm import tqdm\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbeb4eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "< naver 뉴스 전문 가져오기 >_select 사용\n",
    "- 네이버 뉴스만 가져와서 결과값 조금 작음 \n",
    "- 결과 메모장 저장 -> 엑셀로 저장 \n",
    "\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "def get_news(n_url):\n",
    "    \n",
    "    news_detail = []\n",
    "    time.sleep(0.1)\n",
    "    headers = {\"User-Agent\":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "    resp = requests.get(n_url, headers=headers)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        \n",
    "        title = soup.select('#articleTitle')[0].text\n",
    "        news_detail.append(title)\n",
    "#         print(title)\n",
    "    \n",
    "        report = soup.select('#articleBodyContents')[0].text\n",
    "        news_detail.append(report)\n",
    "#         print(report)\n",
    "    \n",
    "        company1 = soup.select('#main_content > div.article_header > div.press_logo > a > img')[0]\n",
    "        company = company1.get('title')\n",
    "        news_detail.append(company)\n",
    "#         print(company)\n",
    "\n",
    "        date = soup.select('#main_content > div.article_header > div.article_info > div > span.t11')[0].text.split(' ')[0]\n",
    "        news_detail.append(date)\n",
    "#         print(date)\n",
    "    \n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    return news_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e192d30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 출력할 페이지수 입력하시오: 400\n",
      "검색어 입력: CJ제일제당\n",
      "시작날짜 입력(2019.01.01):2016.01.01\n",
      "끝날짜 입력(2019.04.28):2016.12.31\n"
     ]
    }
   ],
   "source": [
    "def crawler(maxpage,query,s_date,e_date):\n",
    "\n",
    "    s_from = s_date.replace(\".\",\"\")\n",
    "    e_to = e_date.replace(\".\",\"\")\n",
    "    page = 1\n",
    "    maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "\n",
    "    all_urls = []\n",
    "    real_all_urls = []\n",
    "    \n",
    "    \n",
    "    while page < maxpage_t:\n",
    "    \n",
    "        #print(page)\n",
    "    \n",
    "\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\"+ query +\"&sort=2&photo=3&field=0&pd=3&ds=\"+ s_date +'&de='+ e_date +'&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from'+ s_from +'to'+ e_to +',a:all&start='+ str(page)\n",
    "        \n",
    "        req = requests.get(url)\n",
    "        #print(url)\n",
    "        cont = req.content\n",
    "        soup = BeautifulSoup(cont, 'html.parser')\n",
    "        #print(soup)\n",
    "        \n",
    "        \n",
    "        for urls in soup.select(\".info\"):\n",
    "            try :\n",
    "#                 print(urls[\"href\"])\n",
    "                if urls[\"href\"].startswith(\"https://news.naver.com\"):\n",
    "                    all_urls.append(urls[\"href\"])\n",
    "#                     print(urls[\"href\"])\n",
    "\n",
    "                    \n",
    "#                     print(all_urls)\n",
    "                    \n",
    "                    \n",
    "            except Exception as e:\n",
    "#                 print(e)\n",
    "                continue\n",
    "                \n",
    "        page += 10\n",
    "    \n",
    "    real_all_urls.extend(all_urls)\n",
    "#     print(real_all_urls)\n",
    "\n",
    "    add_news_dict = []\n",
    "    for i in real_all_urls:\n",
    "        \n",
    "        news_detail = get_news(i)\n",
    "        \n",
    "        try:\n",
    "        \n",
    "    \n",
    "            news_dict = {'title' : news_detail[0],\n",
    "                         'report' : news_detail[1],\n",
    "                         'company' : news_detail[2],\n",
    "                         'date' : news_detail[3]}\n",
    "#             print(news_dict)\n",
    "\n",
    "            add_news_dict.append(news_dict)\n",
    "        except:\n",
    "            pass\n",
    "#     print(add_news_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "    news_df = pd.DataFrame(add_news_dict)\n",
    "    folder_path = os.getcwd()\n",
    "    xlsx_file_name = '네이버뉴스_{}_{}_{}.xlsx'.format(query,s_date,e_date)\n",
    "\n",
    "    news_df.to_excel(xlsx_file_name)\n",
    "                    \n",
    "\n",
    "\n",
    "def main():\n",
    "    maxpage = input(\"최대 출력할 페이지수 입력하시오: \") \n",
    "    query = input(\"검색어 입력: \")\n",
    "    s_date = input(\"시작날짜 입력(2019.01.01):\")  #2019.01.01\n",
    "    e_date = input(\"끝날짜 입력(2019.04.28):\")   #2019.04.28\n",
    "    crawler(maxpage,query,s_date,e_date) #검색된 네이버뉴스의 기사내용을 크롤링합니다. \n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66d42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e910a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
